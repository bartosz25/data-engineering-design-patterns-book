# Data glossary

Terms referenced in the book, just a quick reminder.

## A

### Aggregation
The operation that combines multiple values together. An example of it is a `COUNT` function that reduces 
several rows into a single value.

### Analytical data
Dataset resulting from analytical calculations made on top of the [transactional data](#transactional-data). Often
it's an aggregated vision for the transactional information.

### At-least once (delivery semantics)
A delivery semantic where the data can be duplicated.

### At-most once (delivery semantics)
A delivery semantic where some records retrieved from the data source can not be written to the output data store.

## B

### Backfilling
The action of processing past data that haven't been processed yet. For example, it's useful for 
processing the data generated before creating an [ETL](#etl) pipeline.


### Backpressure
A throughput-control mechanism. If there is too many records to process, data consumer can ask the input data 
store to apply a _backpressure_ strategy that can be slowing down the data ingestion. Eventually, a consumer can 
use a different approach and read the incoming records, process only a fixed subset of them, and save the extra ones
on a backpressure storage.

## C

### Cluster
A set of [node](#nodes)s used to perform a distributed computation.

### Commit log
A data structure that logs every operation on a data store, such as a new `INSERT` or `UPDATE` operations.

## D

### DAG
An acronym for Directed Acyclic Graph. It represents a graph with directions (i.e. moves from one node to another)
and without cycles (i.e. a given path is not visited twice).

### DAG (Apache Airflow)
See [DAG](#DAG). In Apache Airflow's context, DAG stands for the data processing workflow composed of 
various sensors and operators.

### Data ingestion
Process of bringing data to a data store.

### Data lake
A data repository storing data in its native and raw format, often by relying on _virtually_ unlimited storage capacities
of an object store. Uses [schema on read](#schema-on-read)

### Data orchestration
Process of coordinating tasks execution. 

### Data profiling
Process of creating a profile (aka summary) for a dataset. The profile describes the dataset with the properties such as 
values distribution, emptiness proportion per column, or number of rows.

### Data warehouse
A structured data store used for reporting and data analytics purposes. Uses [schema on write](#schema-on-write)

### Deletes (hard)
The data removal action that physically removes a row from disk, thus from table. 
It can be considered as a _physical data removal_.

### Deletes (soft)
The data removal action that marks a row as being removed but doesn't remove it from disk. In other words,
the row is still present in the table and can still be read by the consumers. It can be considered as a 
_logical data removal_.

## E

### ELT
An acronym for Extract Load Transform. These three words describe the main steps of a data pipeline that includes
data retrieval, data loading to the output data store, and finally, the transformation on top of the loaded data.

### ETL
An acronym for Extract Transform Load.  Unlike [ELT](#ELT), ETL performs the transformation outside the final data 
store.

### Exactly-once (delivery semantics)
A delivery semantic where the data is delivered only once.

## F

### Fail-fast
An approach in software engineering that favors early failure of a program to shorten the feedback loop.

## I

### Idempotence
Property that guarantees the dataset generated by a data producer is written effectively once, even though the 
producer itself has some retries.

### Immutability
The property describing a system which doesn't accept in-place changes. In other words, the data can only be added.

### Incremental data processing
Data processing that doesn't involve the full dataset. Instead, it only addresses new or updated rows, greatly reducing
the data volume.

## L

### Lakehouse
A  data storage paradigm mixing the consistency and data management practices of a data warehouse and storage flexibility 
of a data lake.

## M

### Master dataset
The dataset that provides context for the [transactional data](#transactional-data). For example, if you consider
an order as a transactional data occurrence, the buyer and product information will be part of the Master dataset.  

### Medallion architecture
Data architecture layout where data is stored in one of three spaces called:
* _Bronze_ - place for the raw data. The records must be stored as is, without any prior transformation.
* _Silver_ - place for the transformed raw data, that might eventually be enriched by other datasets. Silver datasets
are not exposed for end users.
* _Gold_ - place for the datasets that can be shared with the end users, such as Machine Learning pipelines, or  
BI dashboards.  

## N

### Node
A server participating in the distributed computation or storage. It's part of the [cluster](#cluster).

## O

### Offline data ingestion
Term used to describe periodical data ingestion, for example a batch data ingestion from a job running every morning.

### Online data ingestion
Term used to describe data ingestion close to the data generation, for example continuous data ingestion from
a streaming job for a stream of events generated without interruption.

## P

### Pipeline
A sequence of actions defining a data processing logic. 

### Poison pill message
A familiar name used to describe events or rows that make a data processing job fail.

### Polyglot persistence
Defines an architecture where a storage use is driven by the end use. For instance, 
a polyglot persistence system can use Elasticsearch to provide some search engine capabilities, 
Apache Cassandra to provide fast key-based access and GCP BigQuery for the data warehouse capabilities.

## R

### Raw data
Represents data that is not processed at any moment in our data pipeline. In other words, the data have exactly the same format as during its initial generation.

### Reprocessing
The action of processing already processed dataset, for example because of a modified business logic or a 
data quality issues introduced by a buggy pipeline.

## S

### Schema on read
Data processing approach where the schema is defined at reading time. In other words, the data structure for the dataset is not defined at 
writing (producers side) but at reading (consumers) time.  

### Schema on write
The opposite for [schema on read](#schema-on-read). The producers have to know and respect the schema before writing the data. 
For example, they have to respect the schema of a relational database before writing the data. 


### Staging area
A data storage location that is private to a [pipeline](#pipeline). The pipeline often writes the generated dataset 
to the staging area to validate its quality before sharing it with downstream consumers.

### Stateful data processing
A job that requires an external dataset state for processing. A great example here is a windowed count in
[incremental data processing](#incremental-data-processing) where a given window can span across different 
job runs. Consequently, you will need to keep a temporary count as a state in a separate place called _state store_.


### Stateless data processing
A job that doesn't rely on the past observations. For example, a simple `SELECT COUNT(*) FROM table` doesn't need
to know anything but the current data to get the right answer.

### Straggler
In distributed computation this term represents one or more tasks taking more time to finish than the others.

## T

### Transactional data
Data generated by applications. An example here is a user making an order in an e-commerce store. 

## U

### Unbounded dataset
Put differently, an infinite dataset. It's often the case of streaming datasets that continuously get new 
records.